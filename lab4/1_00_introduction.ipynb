{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Data Science Outcomes With Efficient Workflow #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00 - Introduction ##\n",
    "Welcome to the NVIDIA DLI Enhancing Data Science Outcomes With Efficient Workflow workshop. In this workshop, you will learn how to build an accelerated end-to-end data processing pipeline for large datasets. In order to achieve this, we will use NVIDIA's RAPIDS suite of open-source software libraries and Dask. This hands-on lab will provide opportunities to get familiar with each step in the development workflow that will enable you to make customizations that are most appropriate for your own use cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives**\n",
    "<br>\n",
    "In this workshop, you will learn how to: \n",
    "* Develop and deploy an accelerated end-to-end data processing pipeline for large datasets\n",
    "* Scale data science workflows using distributed computing\n",
    "* Perform DataFrame transformations that take advantage of hardware-acceleration and avoid hidden slowdowns\n",
    "* Enhance machine learning solutions through feature engineering and rapid experimentation\n",
    "* Improve data process pipeline performance by optimizing memory management and hardware utilization\n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "This workshop is broken down into three parts: \n",
    "1. Advanced ETL - learn how to efficiently perform extract, transform, and load on large datasets\n",
    "2. Advanced Analytics - learn how to leverage acceleration to improve analytical outcomes\n",
    "3. Deployment - learn how to deploy the data processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JupyterLab ###\n",
    "For this hands-on lab, we use [JupyterLab](https://jupyterlab.readthedocs.io/en/stable/) to manage our environment.  The [JupyterLab Interface](https://jupyterlab.readthedocs.io/en/stable/user/interface.html) is a dashboard that provides access to interactive iPython notebooks, as well as the folder structure of our environment and a terminal window into the Ubuntu operating system. The first view includes a **menu bar** at the top, a **file browser** in the **left sidebar**, and a **main work area** that is initially open to this \"introduction\" notebook. \n",
    "\n",
    "<p><img src=\"images/jl_launcher.png\" width=720></p>\n",
    "\n",
    "* The file browser can be navigated just like any other file explorer. A double click on any of the items will open a new tab with its content. \n",
    "* The main work area includes tabbed views of open files that can be closed, moved, and edited as needed. \n",
    "* The notebooks, including this one, consist of a series of content and code **cells**. To execute code in a code cell, press `Shift+Enter` or the `Run` button in the menu bar above, while a cell is highlighted. Sometimes, a content cell will get switched to editing mode. Executing the cell with `Shift+Enter` or the `Run` button will switch it back to a readable form.\n",
    "* To interrupt cell execution, click the `Stop` button in the menu bar or navigate to the `Kernel` menu, and select `Interrupt Kernel`. \n",
    "* We can use terminal commands in the notebook cells by prepending an exclamation point/bang(`!`) to the beginning of the command. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='e1'></a>\n",
    "#### Exercise #1 - Practice ####\n",
    "Try executing the simple print statement in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is just a simple print statement\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# activate this cell by selecting it with the mouse or arrow keys then use the keyboard shortcut [Shift+Enter] to execute\n",
    "print('This is just a simple print statement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try executing the terminal command in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is another simple print statement\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# activate this cell by selecting it with the mouse or arrow keys then use the keyboard shortcut [Shift+Enter] to execute\n",
    "!echo 'This is another simple print statement'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Developer Tools to Monitor GPU Utilization ###\n",
    "There are several tools that can help us monitor our device(s). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NVIDIA System Management Interface ####\n",
    "The [NVSMI](https://developer.nvidia.com/nvidia-system-management-interface) provides monitoring information for Tesla and select Quadro devices. To use, we can execute the `nvidia-smi` command in a terminal. A useful option for NVSMI is `-l` or `--loop`, which will probe at a specific interval in seconds. Alternatively, we can use the `watch` command with `-n` or `--interval` option to achieve similar results. For example, copy and execute the below command in a new terminal window to run `nvidia-smi` every 15 seconds. While looping, we can enter `Ctrl+C` in the terminal to stop and exit the process. At any time throughout the workshop, we can use this to monitor GPU utilization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"\", data-commandlinker-command=\"terminal:create-new\">Open Terminal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# run this cell, then click the link created to open a terminal\n",
    "from IPython.display import HTML\n",
    "HTML('<a href=\"\", data-commandlinker-command=\"terminal:create-new\">Open Terminal</a>')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "watch -n 15 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "For this workshop, we have been given access to 4 T4 GPUs, which is equipped with 16GB device memory each. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='dask_dashboard_diagnostics'></a>\n",
    "#### Dask Dashboard Diagnostics and Dask JupyterLab Extension ####\n",
    "\n",
    "Dask is a flexible library for parallel computing in Python, which we will learn and use throughout the workshop. Profiling parallel code can be challenging, but the interactive dashboard provided with Dask's distributed scheduler makes this easier with live monitoring of Dask computations. Dask offers various diagnostics to help us understand the performance characteristics of the cluster, including hardware utilization. There are numerous diagnostic plots available. Below are some of the most commonly used plots, which are shown on the entry point for the dashboard: \n",
    "* **Bytes Stored and Bytes per Worker** shows a summary of the overall memory usage on the cluster as _Bytes Stored_, as well as the individual usage on each worker as _Bytes per Worker_. \n",
    "* **Task Processing** shows the number of tasks that have been assigned to each worker, even if they are not being processed at the moment. Tasks assigned to the worker(s) will wait to run, depending on their priority and whether their dependencies are in memory. If any of the bars is empty, it could mean that the distribution of tasks across workers is not optimized. \n",
    "* **CPU Utilization/Occupancy** shows CPU usage per worker and the occupancy, which is the amount of time Dask expects it would take to run all the tasks, per worker. \n",
    "* **Task Stream** shows all tasks across worker-threads, where each row represents a thread, and each rectangle represents an individual task. \n",
    "* **Progress** shows the progress of a set of tasks, where the colors of the bars represent different task-prefix and matches with those shown in the Task Stream plot. \n",
    "\n",
    "For example, below we show what the Dask Dashboard Diagnostics looks like. This can be used throughout the lab to monitor Dask computations but is not available until a Dask cluster has been created. Separately, we've also enabled the [Dask JupyerLab Extension](https://github.com/dask/dask-labextension), which allows us to choose any of Dask's dashboard plots and integrate as a pane in the session. More information about Dashboard Diagnostics can be found in the [documentation](https://docs.dask.org/en/stable/dashboard.html#). \n",
    "\n",
    "<p><img src='images/dask_diagnostics_1.PNG' width=600></p>\n",
    "<p><img src='images/dask_diagnostics_2.PNG' width=600></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='e2'></a>\n",
    "#### Exercise #2 - Kernel Shutdown ####\n",
    "Occasionally, we may need to shut down our session kernel to reset the memory. We can see the running kernels using the menu: \n",
    "\n",
    "<p><img src='images/kernel.PNG' width=360></p>\n",
    "\n",
    "We can choose _Shut Down Kernel_ from the menu. Alternatively, below cell contains the code to shut down the current kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shutdown kernel\n",
    "import IPython\n",
    "\n",
    "app=IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well Done!** Let's move to the [next notebook](3_01_model_deployment_for_inference.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
